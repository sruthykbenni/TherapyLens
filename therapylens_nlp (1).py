# -*- coding: utf-8 -*-
"""TherapyLens_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TXmG64kfhtG_slhrBPXtl5dH3i7kn46Z
"""

# STEP 1: Install Required Libraries
'''!pip install transformers sentencepiece textblob wordcloud bertopic scikit-learn umap-learn langdetect -q'''

# STEP 2: Import Libraries
import json
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from bertopic import BERTopic
from langdetect import detect
import nltk
nltk.download('punkt')

# STEP 3: Upload Dataset
from google.colab import files
uploaded = files.upload()  # Upload intents.json file

# STEP 4: Load and Prepare Data
import io
data = json.load(io.StringIO(uploaded['intents.json'].decode('utf-8')))
entries = []
for intent in data['intents']:
    for pattern in intent['patterns']:
        entries.append({'text': pattern, 'tag': intent.get('tag', 'unknown')})
df = pd.DataFrame(entries)
df['session'] = (df.index // 10) + 1

# STEP 5: Multilingual Detection (Optional Filtering)
from langdetect import detect
df['language'] = df['text'].apply(lambda x: detect(x))
df = df[df['language'] == 'en']  # Keeping only English entries

# STEP 6: Sentiment & Emotion Detection
sentiment_pipeline = pipeline("sentiment-analysis")
emotion_pipeline = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=1)
df['sentiment_result'] = df['text'].apply(lambda x: sentiment_pipeline(x)[0])
df['sentiment'] = df['sentiment_result'].apply(lambda x: x['label'])
df['sentiment_score'] = df['sentiment_result'].apply(lambda x: x['score'])
df['emotion_result'] = df['text'].apply(lambda x: emotion_pipeline(x)[0][0])
df['emotion'] = df['emotion_result'].apply(lambda x: x['label'])
df['emotion_score'] = df['emotion_result'].apply(lambda x: x['score'])

# STEP 7: Emotion Intensity Categorization
def categorize_intensity(score):
    if score >= 0.85: return "Extreme"
    elif score >= 0.65: return "Strong"
    elif score >= 0.4: return "Moderate"
    else: return "Mild"
df['emotion_intensity'] = df['emotion_score'].apply(categorize_intensity)

# STEP 8: Topic Modeling with BERTopic
topic_model = BERTopic()
topics, _ = topic_model.fit_transform(df['text'])
df['topic'] = topics

# STEP 9: Session Summarization with T5
from transformers import T5Tokenizer, T5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

def summarize(text):
    input_text = "summarize: " + text
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=50, min_length=10, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

df['session_text'] = df.groupby('session')['text'].transform(lambda x: " ".join(x))
df['session_summary'] = df.groupby('session')['session_text'].transform(lambda x: summarize(x.iloc[0]) if len(x.iloc[0].split()) > 20 else "Too short for summary")

# STEP 10: Critical Emotion Alert System
def alert_flag(row):
    if row['emotion'] in ['anger', 'sadness'] and row['emotion_score'] > 0.8:
        return "ALERT"
    return "NORMAL"
df['alert'] = df.apply(alert_flag, axis=1)
df['consec_alert'] = df['alert'].eq('ALERT').astype(int)
df['consec_alert'] = df['consec_alert'].groupby(df['session']).transform('sum')
df['session_alert'] = df['consec_alert'].apply(lambda x: 'CRITICAL' if x >= 3 else 'OK')

# STEP 11: Visualizations
sentiment_trend = df.groupby("session")["sentiment"].value_counts().unstack().fillna(0)
emotion_trend = df.groupby("session")["emotion"].value_counts().unstack().fillna(0)

sentiment_trend.plot(title="Sentiment Trend Across Sessions", figsize=(10,5))
plt.grid(True)
plt.show()

emotion_trend.plot(title="Emotion Trend Across Sessions", figsize=(10,5))
plt.grid(True)
plt.show()

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(df['text']))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Diary Entries")
plt.show()

# STEP 12: Export Final Results
df.to_csv("therapy_emotion_analysis_advanced.csv", index=False)
print("âœ… Final results saved to 'therapy_emotion_analysis_advanced.csv'")

"""Streamlit App"""

'''!pip install streamlit -q'''

# app.py

import json
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from transformers import pipeline
from bertopic import BERTopic
from langdetect import detect
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load models
sentiment_pipeline = pipeline("sentiment-analysis")
emotion_pipeline = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=1)
summarizer_tokenizer = T5Tokenizer.from_pretrained("t5-small")
summarizer_model = T5ForConditionalGeneration.from_pretrained("t5-small")
topic_model = BERTopic()

# Function: summarize text
def summarize(text):
    input_text = "summarize: " + text
    input_ids = summarizer_tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = summarizer_model.generate(input_ids, max_length=50, min_length=10, num_beams=4, early_stopping=True)
    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Streamlit UI
st.title("ğŸ§  Therapy Emotion Tracker Dashboard")
st.markdown("Analyze diary-style therapy entries for emotional trends, summaries, alerts, and more.")

uploaded_file = st.file_uploader("/content/intents.json", type="json")

if uploaded_file:
    raw = json.load(uploaded_file)
    data = []
    for intent in raw['intents']:
        for pattern in intent['patterns']:
            data.append({"text": pattern, "tag": intent.get("tag", "unknown")})

    df = pd.DataFrame(data)
    df['session'] = (df.index // 10) + 1
    df['language'] = df['text'].apply(lambda x: detect(x))
    df = df[df['language'] == 'en']

    # Analyze
    df['sentiment_result'] = df['text'].apply(lambda x: sentiment_pipeline(x)[0])
    df['sentiment'] = df['sentiment_result'].apply(lambda x: x['label'])
    df['sentiment_score'] = df['sentiment_result'].apply(lambda x: x['score'])
    df['emotion_result'] = df['text'].apply(lambda x: emotion_pipeline(x)[0][0])
    df['emotion'] = df['emotion_result'].apply(lambda x: x['label'])
    df['emotion_score'] = df['emotion_result'].apply(lambda x: x['score'])

    # Emotion Intensity
    def intensity(score):
        if score >= 0.85: return "Extreme"
        elif score >= 0.65: return "Strong"
        elif score >= 0.4: return "Moderate"
        else: return "Mild"
    df['emotion_intensity'] = df['emotion_score'].apply(intensity)

    # Summarization
    df['session_text'] = df.groupby('session')['text'].transform(lambda x: " ".join(x))
    df['session_summary'] = df.groupby('session')['session_text'].transform(
        lambda x: summarize(x.iloc[0]) if len(x.iloc[0].split()) > 20 else "Too short")

    # Topic Modeling
    topics, _ = topic_model.fit_transform(df['text'])
    df['topic'] = topics

    # Alerts
    def alert_flag(row):
        if row['emotion'] in ['anger', 'sadness'] and row['emotion_score'] > 0.8:
            return "ALERT"
        return "NORMAL"
    df['alert'] = df.apply(alert_flag, axis=1)
    df['session_alert'] = df.groupby('session')['alert'].transform(lambda x: "CRITICAL" if (x == 'ALERT').sum() >= 3 else "OK")

    # Dashboard Panels
    st.subheader("ğŸ“Š Sentiment & Emotion Overview")
    st.dataframe(df[['text', 'sentiment', 'emotion', 'emotion_intensity', 'alert']])

    st.subheader("ğŸ“ˆ Emotional Trend by Session")
    trend = df.groupby("session")["emotion"].value_counts().unstack().fillna(0)
    st.line_chart(trend)

    st.subheader("ğŸ§µ Session Summaries")
    for session in df['session'].unique():
        summary = df[df['session'] == session].iloc[0]['session_summary']
        st.markdown(f"**Session {session}**: {summary}")

    st.subheader("ğŸŒ Word Cloud")
    wc = WordCloud(width=800, height=300, background_color='white').generate(" ".join(df['text']))
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)

    st.subheader("ğŸš¨ Critical Session Alerts")
    criticals = df[df['session_alert'] == 'CRITICAL'][['session', 'emotion', 'emotion_score']]
    if not criticals.empty:
        st.warning("âš ï¸ Critical emotions detected:")
        st.dataframe(criticals)
    else:
        st.success("âœ… No critical emotion patterns detected.")
